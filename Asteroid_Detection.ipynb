{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18eadfae-8fa1-48df-bf94-498cd2296e9d",
   "metadata": {},
   "source": [
    "# Generate Asteroid CSV for Calibration Level 2 Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982df571-d061-43e5-a705-d2615a577af4",
   "metadata": {},
   "source": [
    "This pipeline iterates through observations within the MIRI database to identify serendipitously detected asteroids. To optimize computational efficiency, it first scans the Level 3 database using the SBIDENT cone search method. Next, the pipeline reviews corresponding Level 2 member observations to avoid dithering effects that can reduce asteroid flux measurements. A final list of detected asteroids is generated, incorporating all relevant observations. Asteroids that do not meet the threshold criteria for accurate flux measurement are then excluded in the 'Asteroid_Analysis.csv' to identify usable flux measurments.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca155fa-75a5-4374-859c-976da41c0b3d",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e870ba2-8a59-4258-b3b4-1fe9c409aa5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import glob\n",
    "\n",
    "import shapely.wkt\n",
    "from shapely.geometry import Point\n",
    "\n",
    "from PIL import Image\n",
    "from sbident import SBIdent\n",
    "\n",
    "import numpy as np\n",
    "import math as mt\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Circle\n",
    "\n",
    "from astropy.io import fits\n",
    "from astropy.wcs import WCS\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy.time import Time\n",
    "import astropy.units as u\n",
    "from astropy.visualization import simple_norm\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from astroquery.jplhorizons import Horizons\n",
    "from astroquery.esa.jwst import Jwst\n",
    "\n",
    "from scipy.ndimage import label\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0144fadc-7440-4b78-b5ac-6ebd49456b4c",
   "metadata": {},
   "source": [
    "## Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "820409be-cea1-40d6-ba7f-10d559f0fcbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def is_in_array(pixel, array):\n",
    "    return any((pixel == x).all() for x in array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8c04583-87ea-49b6-832b-b65bc6624e8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def MJDconversion(modifiedJulianDate):\n",
    "    # Convert string from Modified Julian Date to YYYMMDD [HH:MM:SS.SS] format\n",
    "    return (Time(modifiedJulianDate, format='mjd').iso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "144c4fe9-a29e-4a0b-8e6f-f74a64a2378c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def combine_strings(string_list):\n",
    "    # Convert a list of strings into a single string comma seperated\n",
    "    return ', '.join(string_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e147104c-6d6e-4cb0-a108-85f0e7c2e434",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generateFolder(folderName):\n",
    "    # Check if folder exists, and if not generates a folder\n",
    "    if not os.path.exists(folderName):\n",
    "        os.mkdir(folderName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e4277f4-b3a5-4505-be98-92922a3805f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def members_string(members_init_string):\n",
    "    # Convert 'members' string from the archive query into a more presentable fashion of level 3 CSV\n",
    "    members_string = members_init_string.replace('caom:JWST/', '').replace(' ',', ')\n",
    "    return (members_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f9632bf-f4cf-4cb7-9bdc-a86cfce676f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def checkDataExists(proposal, observation):\n",
    "    # Check if the observation is on Datalabs\n",
    "    jwst_file = f\"jw0{next(c for c in proposal if c != '0')}\"\n",
    "    dataPath = f'/data/user/jwst_{jwst_file}/jw0{proposal}/{observation}_i2d.fits.gz'    \n",
    "    return (os.path.exists(dataPath))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e4e1e48-0aa3-4d35-8c10-ca83d9a63ed2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pullWCS(imagePath):\n",
    "    # Open fits image, retireving header information (including WCS) and image array\n",
    "    with fits.open(imagePath) as hdul:\n",
    "        main_header = hdul[0].header\n",
    "        header = hdul[1].header\n",
    "        data = hdul[1].data\n",
    "        wcs_info = WCS(header)\n",
    "    \n",
    "    return(data, header, wcs_info, main_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a3689f6-4d8e-4735-ba82-4a4a51076e54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HiddenPrints:\n",
    "    # Does not produce print outputs, used for built in functions with noisy print statmeents\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = self._original_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9725196-2b90-4437-9b2f-b85b28d533f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def formatPolygon(polyString):\n",
    "    # Format the archive polygon string to a format that is compatible with the shapely function\n",
    "    #slice away the polygon charactors 'polygon((' from the start and '))' from the end\n",
    "    coords = polyString[8:-2].split(' ')\n",
    "    \n",
    "    #Add in the fist location at the end to close the 4 point region, shapely expects 5 coordinates\n",
    "    coords.append(coords[0])\n",
    "    coords.append(coords[1])\n",
    "    \n",
    "    return f\"POLYGON (({', '.join([coords[i] + ' ' + coords[i+1] for i in range(0, len(coords), 2)])}))\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abab3c95-000e-4d44-81e5-7687736f7ede",
   "metadata": {},
   "source": [
    "## Archive Query Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d75f0b4d-7d61-4541-9e2c-c9b86f7ff4c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def queryArchive(volume, readouts, query_filters):\n",
    "    # Generate an adql query search for the JWST archive to filter the observations and produce a pandas DF containing all useful information\n",
    "    \n",
    "    # Set up constraints and filters for data selected for the archive to return\n",
    "    query_string = f\"SELECT {','.join(readouts)} FROM jwst.{volume} WHERE {' AND '.join(query_filters)}\"\n",
    "    \n",
    "    # Run job and convert archive results to a pandas dataframe \n",
    "    job = Jwst.launch_job(query_string, async_job=True)\n",
    "    panda_result = job.get_results().to_pandas()\n",
    "        \n",
    "    #Sort the dataframe by the proposal ID\n",
    "    return panda_result.sort_values(by=['observationid']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa197830-4c75-4360-842d-094f95845493",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def level3_Archive_Query(propRange = [1000,9000], instrumentName = 'MIRI/IMAGE', dataType = 'image', volume = 'archive', additionalFilters = []):\n",
    "    # Define the filters and outputs fro the ADQL query\n",
    "    \n",
    "    lowerbound_proposal, upperbound_proposal = propRange\n",
    "    \n",
    "    calLVL = 3\n",
    "    \n",
    "    # Define query filters\n",
    "    query_filters = [\n",
    "            f'jwst.{volume}.calibrationlevel = {calLVL}',\n",
    "            f\"jwst.{volume}.dataproducttype = '{dataType}'\",\n",
    "            f\"jwst.{volume}.instrument_name = '{instrumentName}'\",\n",
    "            f\"jwst.{volume}.proposal_id >= '{lowerbound_proposal}'\",\n",
    "            f\"jwst.{volume}.proposal_id <= '{upperbound_proposal}'\",\n",
    "        ] + additionalFilters  # Append additional filters if provided\n",
    "    \n",
    "    # Define which archive outputs of interest\n",
    "    query_topics = ['proposal_id',  'observationid', 'dataproducttype', 'intent',  'instrument_name',  'energy_bandpassname',\n",
    "                   'target_moving','position_bounds_spoly','time_bounds_lower','time_bounds_upper','members']\n",
    "    \n",
    "    # Run the search, return a dataframe with results\n",
    "    with HiddenPrints():\n",
    "        level3_ArchiveDF = queryArchive(volume, query_topics, query_filters)\n",
    "    \n",
    "    return (level3_ArchiveDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c685386-a46c-42df-8119-312f17e1cad9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def level2_Archive_Query(observation_list, instrumentName = 'MIRI/IMAGE', dataType = 'image', volume = 'archive', additionalFilters = []):\n",
    "        \n",
    "    # Join observations into a string list to search only those observations in the archive\n",
    "    observation_list_string = ', '.join([f\"'{obsid}'\" for obsid in observation_list])\n",
    "    \n",
    "    calLVL = 2\n",
    "    \n",
    "    # Define query filters\n",
    "    query_filters = [\n",
    "            f'jwst.{volume}.observationid IN ({observation_list_string})',\n",
    "            f'jwst.{volume}.calibrationlevel = {calLVL}',\n",
    "            f\"jwst.{volume}.dataproducttype = '{dataType}'\",\n",
    "            f\"jwst.{volume}.instrument_name = '{instrumentName}'\"\n",
    "        ] + additionalFilters  # Append additional filters if provided\n",
    "    \n",
    "    # Define which archive outputs of interest\n",
    "    query_topics = ['proposal_id',  'observationid', 'dataproducttype', 'intent',  'instrument_name',  'energy_bandpassname',\n",
    "                   'target_moving','target_name','position_bounds_spoly','time_bounds_lower','time_bounds_upper']\n",
    "    \n",
    "    #Run the search, return a dataframe with results\n",
    "    with HiddenPrints():\n",
    "        level2_ArchiveDF = queryArchive(volume, query_topics, query_filters)\n",
    "    \n",
    "    return(level2_ArchiveDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351b06ff-c0ac-4db2-b53a-d1a3d9cca650",
   "metadata": {},
   "source": [
    "## Small Body Identification Cone Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c08881c9-9967-45f3-9cf0-fbe1dda2ceaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def definePolyEdges(polyString):\n",
    "    # Define the image boundaries for the cone search\n",
    "    # NOTE: the cone search takes RA and DEC bounds so image bounds are aligned with the DEC and RA axis (using the min,max bounds)\n",
    "    \n",
    "    # Deconstruct the polygon string into its RA and DEC coordinates \n",
    "    coordinates = polyString.replace(\"POLYGON ((\", \"\").replace(\"))\", \"\").replace(\", \", \" \").split()\n",
    "    coordinates = list(map(float, coordinates))\n",
    "    \n",
    "    RA_elements = coordinates[::2]\n",
    "    DEC_elements = coordinates[1::2]\n",
    "    \n",
    "    # Add in a small buffer around the image edges\n",
    "    buffer = 0.005  #Deg\n",
    "\n",
    "    # Identify the max/min boundary range of the image by identifying 2 corners of the image\n",
    "    low_right_corner = SkyCoord(min(RA_elements) - buffer, min(DEC_elements) - buffer, frame='icrs', unit='deg')\n",
    "    up_left_corner   = SkyCoord(max(RA_elements) + buffer, max(DEC_elements) + buffer, frame='icrs', unit='deg')\n",
    "    \n",
    "    return([low_right_corner, up_left_corner])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30307082-d966-4692-9258-cd0b00cc90f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def expProbeTime(expStartMJD, expEndMJD):\n",
    "    # For longer exposure times, define probe times to cone search for streaking asteroids\n",
    "    probe_time = (1/3)/24 #20 minutes\n",
    "    \n",
    "    # Check how many 20 minute segments are in the exposure time\n",
    "    probe = expStartMJD  + probe_time\n",
    "    \n",
    "    if (expEndMJD - expStartMJD) > probe:\n",
    "        probe_list = [expStartMJD]\n",
    "        \n",
    "        while probe < expEndMJD:\n",
    "            probe_list.append(MJDconversion(probe + probe_time))\n",
    "            probe += probe_time\n",
    "        \n",
    "    else:\n",
    "        # Choose the center of the exposure time\n",
    "        probe_list = [MJDconversion(expStartMJD + (expEndMJD - expStartMJD)/2)]\n",
    "        \n",
    "    # Return a list containing all of the 20 minute times during the exposure time to be cone searched over\n",
    "    return (probe_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b52bf562-9a79-495d-9309-1bce760e9020",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def JWSTposition(obsTime):\n",
    "    # Determine the position of JWST during the observation time from an Earth perspective\n",
    "    # Follows example 3 from https://github.com/bengebre/sbident/blob/main/examples/sbident-examples.ipynb \n",
    "    \n",
    "    #NOTE: it is likely that this value can be pulled from the image header in the future\n",
    "    \n",
    "    # Generate AU to km conversion\n",
    "    au_to_km = (1 * u.au).to(u.km).value\n",
    "    \n",
    "    # Probe for the JWST output from JPL Horizons, state vector\n",
    "    jwst_output = Horizons(id='JWST',location='Geocentric',epochs=obsTime.jd, id_type='id').vectors(refplane='earth')\n",
    "\n",
    "    # Convert position and velocity from AU to km and km/s respectively\n",
    "    jwst_output_km = jwst_output[['x', 'y', 'z', 'vx', 'vy', 'vz']].to_pandas().to_numpy()\n",
    "    jwst_output_km[:, :3] *= au_to_km  # Convert position (x, y, z) from AU to km\n",
    "    jwst_output_km[:, 3:] /= 86400     # Convert velocity (vx, vy, vz) from AU/day to km/s\n",
    "\n",
    "    # Form the xobs dictionary that is the input for SBIdent location argument\n",
    "    xobs = ','.join([f\"{s:.12e}\" for s in jwst_output_km[0]])\n",
    "    return {'xobs': xobs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b3089c92-abe7-4b7a-9130-d436f2b452ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def coneSearch(Exptime, Edge1, Edge2):\n",
    "    # Apply cone search method to identify what asteroids are present in the observation at a specific time (exp time) bound by the image corners (ra, dec)\n",
    "    # NOTE: unlike JPL Horizons, the sbident cone search only utilizes 1 fixed time\n",
    "    \n",
    "    # Convert the exposure time string into the observation time to probe the cone search\n",
    "    ObsTime = Time(Exptime)\n",
    "    \n",
    "    # Determine the JWST position at the moment of observation\n",
    "    jwstLocation = JWSTposition(ObsTime)\n",
    "\n",
    "    # Apply the small body identification cone search method 'sbid' from https://github.com/bengebre/sbident\n",
    "    try:\n",
    "        sbid = SBIdent(jwstLocation, ObsTime, [Edge1, Edge2]).results\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Some times the connection gets interrupted and needs to be reran\n",
    "        logging.info(\"Failed First Try in SBIDENT\")\n",
    "        logging.info(e)\n",
    "        time.sleep(5)\n",
    "        \n",
    "        # Try again\n",
    "        try:\n",
    "            sbid = SBIdent(jwstLocation, ObsTime, [Edge1, Edge2]).results\n",
    "        except Exception as e:\n",
    "            logging.info(\"Failed Second Try in SBIDENT\")\n",
    "            logging.info(e)\n",
    "            sbid = False\n",
    "    \n",
    "    # If the return sbid is an empty list convert it to a False boolean\n",
    "    if isinstance(sbid, list) and not sbid:\n",
    "        logging.info(\"SBIDENT Output was empty\")\n",
    "        sbid = False\n",
    "    \n",
    "    return(sbid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2570e52e-5a28-482b-9fdf-28acfb4b11c5",
   "metadata": {},
   "source": [
    "## Level 3 Asteroid Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1f32a84e-81f5-48a4-b904-52e66c942968",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def level3_asteroid_names(expStartMJD, expEndMJD, polyStringFormatted):\n",
    "    # Return all the asteroid names within the level 3 image conesearch\n",
    "    \n",
    "    # Define the image bounds from the polygon\n",
    "    poly_corners = definePolyEdges(polyStringFormatted)\n",
    "    \n",
    "    # Define the probe time for individual searches\n",
    "    probeList = expProbeTime(expStartMJD, expEndMJD)\n",
    "    \n",
    "    asteroid_names = []\n",
    "    \n",
    "    # Loop over the probe times within an image and return all asteroid names from the conesearch\n",
    "    for probe in probeList:\n",
    "        sbid_middle_results = coneSearch(probe, *poly_corners)\n",
    "        \n",
    "        if sbid_middle_results:\n",
    "            asteroid_names.append(sbid_middle_results['Object name']) \n",
    "        else:\n",
    "            # No asteroids found in the cone search\n",
    "            pass\n",
    "    \n",
    "    # Find only uniqely contained Asteroids\n",
    "    unique_asteroids = list(set(item.split('(')[-1].replace(')', '') for sublist in asteroid_names for item in sublist))\n",
    "\n",
    "    if len(unique_asteroids) > 0:\n",
    "        return(', '.join([f\"{ast}\" for ast in unique_asteroids]))\n",
    "\n",
    "    else:\n",
    "        # No Asteroids found\n",
    "        return('')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a01ffbff-b405-454a-b6ce-04094c6f39cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def level3_asteroid_search(row):\n",
    "    #Begin the asteroid search process \n",
    "    \n",
    "    proposal = row['proposal_id']\n",
    "    observation = row['observationid']\n",
    "    polygonString = row['position_bounds_spoly']\n",
    "    expStartMJD = row['time_bounds_lower']\n",
    "    expEndMJD = row['time_bounds_upper']\n",
    "\n",
    "    #convert the format of the polygon string\n",
    "    poly_string_formatted = formatPolygon(str(polygonString))\n",
    "    \n",
    "    #perform the initial Cone Search\n",
    "    sbident_asteroids = level3_asteroid_names(expStartMJD, expEndMJD, poly_string_formatted)\n",
    "        \n",
    "    return(sbident_asteroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f8f526-c6d2-4921-a0b5-512a37947b58",
   "metadata": {},
   "source": [
    "## JPL Horizons Comparitive Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c6679318-dc7b-47cf-8ebb-efd135a29ffd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def jplHorizonsSearch(targetID, startTime, stopTime, polyString):\n",
    "    #Search the JPL Horizons data for a specific target to get orbital values. \n",
    "    #This method is more accurate then the cone search and provides a double check for asteroids (named from the cone search) existing in the image\n",
    "    \n",
    "    #generate polygon variable from the polygon string\n",
    "    poly_shape = shapely.wkt.loads(polyString)\n",
    "    \n",
    "    #define the probe minutes at its lowest setting as to not miss ny additional data\n",
    "    #Note: there is likely a way to toggle this depending on the length of the exposure time as sometimes the exposure tis very long\n",
    "\n",
    "    probeMinutes = 1\n",
    "    #get return from the JPL horizons output regarding the specific target searched\n",
    "    try:\n",
    "        jpl_output = Horizons(\n",
    "            id=targetID, \n",
    "            location='Geocentric@JWST', \n",
    "            epochs={'start': str(startTime), 'stop' : str(stopTime), 'step' : f\"{probeMinutes}m\"})\n",
    "        \n",
    "        #get Ephemerides data\n",
    "        jpl_pandas = jpl_output.ephemerides().to_pandas()\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.info(f\"JPL Horizons Failed to retrieve data for {targetID}, Trying again\")\n",
    "        time.sleep(30)\n",
    "        if targetID == '81P/Wild 2':\n",
    "            targetID = '90000862'\n",
    "        \n",
    "        try:           \n",
    "            jpl_output = Horizons(\n",
    "            id=targetID, \n",
    "            id_type='smallbody',\n",
    "            location='Geocentric@JWST', \n",
    "            epochs={'start': str(startTime), 'stop' : str(stopTime), 'step' : f\"{probeMinutes}m\"}) \n",
    "            \n",
    "            #get Ephemerides data\n",
    "            jpl_pandas = jpl_output.ephemerides().to_pandas()\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.info(f\"JPL Horizons Failed Second Try for {targetID}\")\n",
    "            logging.info(e)\n",
    "            return(False, False)     \n",
    "    \n",
    "    #check to see if there are any times during this that the asteroid intercepts with the imaging window\n",
    "    return (any(poly_shape.contains(Point(ra, dec)) for ra, dec in zip(jpl_pandas['RA'], jpl_pandas['DEC'])), jpl_pandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbd0c9b-7684-4e44-909f-90cdc59ee7ed",
   "metadata": {},
   "source": [
    "## Level 2 Asteroid Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fd77ebd7-4455-4dbd-b4c2-7a98da85733a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_asteroid_center(imageData, image_parameters, jpl_x, jpl_y, uncertainty, image_filter):\n",
    "    # Find the center of the asteroid\n",
    "    \n",
    "    filters_psf = { \"F560W\":  [3.87,  10.78, 18.39, 1.436],\n",
    "                    \"F770W\":  [4.22,  8.92,  14.64, 1.442],\n",
    "                    \"F1000W\": [4.60,  6.70,  11.58, 1.453],\n",
    "                    \"F1130W\": [4.92,  7.06,  11.22, 1.465],\n",
    "                    \"F1280W\": [5.18,  7.61,  10.99, 1.483],\n",
    "                    \"F1500W\": [5.69,  8.63,  11.45, 1.497],\n",
    "                    \"F1800W\": [6.29,  10.09, 12.52, 1.506],\n",
    "                    \"F2100W\": [7.91,  11.90, 16.07, 1.492],\n",
    "                    \"F2550W\": [9.18,  14.03, 17.71, 1.506],\n",
    "                    \"FND\":    [5.27,  8.98,  13.74, 1.455],\n",
    "                    \"F1065C\": [10.07, 17.77, 24.80, 1.464],\n",
    "                    \"F1140C\": [10.68, 19.10, 27.06, 1.461],\n",
    "                    \"F1550C\": [14.21, 25.52, 36.77, 1.459],\n",
    "                    \"F2300C\": [13.27, 16.84, 27.25, 1.470]}\n",
    "\n",
    "    apertureRange, innerAnnulus, outerAnnulus, correctionFactor = filters_psf[image_filter]\n",
    "    \n",
    "    \n",
    "    if uncertainty < 2:\n",
    "        #asteroid is close, requiring a small window to search\n",
    "        extra_pixels = 14\n",
    "        \n",
    "    else:\n",
    "        #asteroid is far, requiring a bigger window to search\n",
    "        extra_pixels = 25\n",
    "    \n",
    "    current_pixel_units, sr_to_pix_conversion, arcsec_to_pix_conversion = image_parameters\n",
    "    \n",
    "    # Crop the image near the asteroid\n",
    "    rows, cols = imageData.shape\n",
    "\n",
    "    # Calculate crop boundaries with bounds checking using numpy.clip and additional buffer pixels\n",
    "    col_start, col_end = np.clip([int(min(jpl_x) - extra_pixels), int(max(jpl_x) + extra_pixels)], 0, cols - 1)\n",
    "    row_start, row_end = np.clip([int(min(jpl_y) - extra_pixels), int(max(jpl_y) + extra_pixels)], 0, rows - 1)\n",
    "\n",
    "    # Extract cropped region\n",
    "    cropped_image = imageData[row_start:row_end, col_start:col_end]\n",
    "\n",
    "    # Adjust X and Y to the cropped image coordinate system\n",
    "    adjusted_asteroid_X = jpl_x - col_start\n",
    "    adjusted_asteroid_Y = jpl_y - row_start\n",
    "\n",
    "    if len(adjusted_asteroid_X) > 1:\n",
    "        dx = abs(adjusted_asteroid_X[-1] - adjusted_asteroid_X[0])\n",
    "        dy = abs(adjusted_asteroid_Y[-1] - adjusted_asteroid_Y[0])\n",
    "        streak_distance_in_pix = mt.sqrt(dx**2 + dy**2)\n",
    "        streak_distance_in_arcsec = streak_distance_in_pix * arcsec_to_pix_conversion\n",
    "        \n",
    "        if streak_distance_in_pix <= 0.9*apertureRange*2:\n",
    "            streakFlag = ''\n",
    "        else:\n",
    "            streakFlag = 'Yes'\n",
    "\n",
    "    else:\n",
    "        streak_distance_in_pix = 'Point'\n",
    "        streakFlag = ''\n",
    "            \n",
    "\n",
    "    # Find the center of the asteroid       \n",
    "    image_shape = cropped_image.shape    \n",
    "        \n",
    "    if len(adjusted_asteroid_X) > 1:\n",
    "        path_difference_ra =  abs(adjusted_asteroid_X[0] - adjusted_asteroid_X[-1])\n",
    "        path_difference_dec = abs(adjusted_asteroid_Y[0] - adjusted_asteroid_Y[-1])\n",
    "            \n",
    "        mid_index = len(adjusted_asteroid_X) // 2\n",
    "        jpl_center_location = [adjusted_asteroid_X[mid_index], adjusted_asteroid_Y[mid_index]]\n",
    "        \n",
    "    else:\n",
    "        jpl_center_location = [adjusted_asteroid_X[0], adjusted_asteroid_Y[0]]\n",
    "\n",
    "        \n",
    "    #Find the median brightness region of the image\n",
    "    bright_center = locate_bright_region(cropped_image, desired_percentile = 99)\n",
    "\n",
    "    #replace any nan values that are connected to the edge of the image to the average of the image as to not skew center location\n",
    "    new_image = replace_nan_edges(cropped_image)\n",
    "        \n",
    "    #select center finding method\n",
    "    if uncertainty < 2:       \n",
    "        #refine the center guess using the starting point from the brightness center\n",
    "        midway_point = ((jpl_center_location[0] + bright_center[0]) / 2, (jpl_center_location[1] + bright_center[1]) / 2)\n",
    "        refined_from_midpoint = refine_center(new_image, midway_point, radius = 12)\n",
    "        \n",
    "    else:    \n",
    "        #refine the center guess using the starting point from the brightness center  \n",
    "        midway_point = ((jpl_center_location[0] + bright_center[0]) / 2, (jpl_center_location[1] + bright_center[1]) / 2)\n",
    "        refined_from_midpoint = refine_center(new_image, midway_point)\n",
    "    \n",
    "    # Calculate the location on the original image\n",
    "    x_center_original = refined_from_midpoint[0] + col_start\n",
    "    y_center_original = refined_from_midpoint[1] + row_start\n",
    "    \n",
    "    center_on_original = np.array([x_center_original,y_center_original])\n",
    "    \n",
    "    # Find difference between the JPL center and the one calculated    \n",
    "    center_dx = abs(jpl_center_location[0] - list(refined_from_midpoint)[0])\n",
    "    center_dy = abs(jpl_center_location[1] - list(refined_from_midpoint)[1])\n",
    "    center_distance_in_pix = mt.sqrt(center_dx**2 + center_dy**2)\n",
    "    center_distance_in_sr = center_distance_in_pix * sr_to_pix_conversion\n",
    "    \n",
    "    return(refined_from_midpoint, center_on_original, streak_distance_in_pix, center_distance_in_sr, streakFlag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7b2786be-65ee-4fce-9e1a-39bfce54c4e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def level2_asteroid_names(expStart, expEnd, filter_name, poly_string_formatted, proposal, observation, asteroid_list, folder_path, produce_images):\n",
    "    \n",
    "    contained_asteroid_names = []\n",
    "    contained_asteroid_ephem = []\n",
    "    \n",
    "    containedAsteroidX = []\n",
    "    containedAsteroidY = []\n",
    "    \n",
    "    asteroids_missing_datalabs = []\n",
    "    asteroids_missing_detector = []\n",
    "    \n",
    "    usefulAsteroids = False\n",
    "    \n",
    "    #check if data exists\n",
    "    jwst_file = f\"jw0{next(c for c in proposal if c != '0')}\"\n",
    "    dataPath = f'/data/user/jwst_{jwst_file}/jw0{proposal}/{observation}_i2d.fits.gz'\n",
    "    \n",
    "    image_on_datalabs = checkDataExists(proposal, observation)\n",
    "            \n",
    "    if image_on_datalabs:\n",
    "        imageData, header, WCS, main_header = pullWCS(dataPath)\n",
    "\n",
    "        for asteroid in asteroid_list:\n",
    "            containedCheck, asteroid_ephem = jplHorizonsSearch(asteroid, expStart, expEnd, poly_string_formatted)\n",
    "\n",
    "            if containedCheck:\n",
    "                asteroid_ra_positions = asteroid_ephem['RA']\n",
    "                asteroid_dec_positions = asteroid_ephem['DEC']\n",
    "\n",
    "                #convert RA and DEC to pixel locations\n",
    "                asteroidPixelX, asteroidPixelY = WCS.all_world2pix(asteroid_ra_positions, asteroid_dec_positions, 0)\n",
    "\n",
    "                #perform the check to ensure the asteroid is on the detector region and will return a useful value\n",
    "                onDetector = onDetectorcheck(imageData, asteroidPixelX, asteroidPixelY)\n",
    "\n",
    "                if onDetector:\n",
    "                    contained_asteroid_names.append(asteroid)\n",
    "                    containedAsteroidX.append(asteroidPixelX)\n",
    "                    containedAsteroidY.append(asteroidPixelY)\n",
    "                    contained_asteroid_ephem.append(asteroid_ephem)\n",
    "                    usefulAsteroids = True\n",
    "\n",
    "                else:\n",
    "                    asteroids_missing_detector.append(asteroid)                \n",
    "        \n",
    "    else:\n",
    "        logging.info(f\"ERROR: Image Data Not Found {dataPath}\")\n",
    "            \n",
    "        \n",
    "    if usefulAsteroids:\n",
    "        if produce_images:\n",
    "            generateFolder(f\"{folder_path}\")\n",
    "        #Determine Image Information\n",
    "        \n",
    "        #PHOTUJA2 = float(header['PHOTUJA2']) #  Flux density (uJy/arcsec2) producing 1 cps\n",
    "        PIXAR_A2 = float(header['PIXAR_A2']) #  Nominal pixel area in arcsec^2\n",
    "        PHOTMJSR = float(header['PHOTMJSR'])  #  Flux density (MJy/steradian) producing 1 cps\n",
    "        PIXAR_SR = float(header['PIXAR_SR'])  #  Nominal pixel area in steradians  \n",
    "        BUNIT    = header['BUNIT']            #  Pixel Units\n",
    "        XPOSURE = float(header['XPOSURE'])    #  Effective exposure time [s]\n",
    "        BITPIX = header['BITPIX']             #  Array data type\n",
    "        \n",
    "        READOUT = main_header.get('READPATT')                                      # Readout Pattern\n",
    "        BRIGHTSKY = f\"{main_header.get('SUBSIZE1')} {main_header.get('SUBSIZE2')}\" # Pixel Array\n",
    "        DITHERTYPE = main_header.get('PATTTYPE')                                   # Dithering Pattern\n",
    "        SUBARRAY = main_header.get('SUBARRAY')                                     # Sub Array Type\n",
    "        \n",
    "        \n",
    "        # Determine Asteroid Specific Values\n",
    "        alpha_values = []\n",
    "        sun_to_asteroid_values = []\n",
    "        asteroid_to_jwst_values = []\n",
    "        position_values = []\n",
    "        position_uncertainty_values = []\n",
    "        position_Rates = []\n",
    "        Visual_Mag = []\n",
    "        Surface_Brightnesses = []\n",
    "        \n",
    "        asteroid_class_list = []\n",
    "        asteroid_radius_list = []\n",
    "        asteroid_albedo_list = []\n",
    "        asteroid_short_name_list = []\n",
    "        \n",
    "        distance_pix = []\n",
    "        distance_arcsec_over_exptime = []\n",
    "        \n",
    "        aperture_flux_mJy_list = []\n",
    "        flux_error_list = []\n",
    "        annulus_median_ratio_list = []\n",
    "        signal_to_noise_list = []\n",
    "        center_offset_sr_list = []\n",
    "        ap_nans = []\n",
    "        ann_nans = []\n",
    "        streak_flags = []\n",
    "        \n",
    "        \n",
    "        for asteroid_ephem in contained_asteroid_ephem:\n",
    "            alpha_values.append(f'{round(np.mean(asteroid_ephem[\"alpha\"]),4)}')                       # Sun-Asteroid-JWST Angle (Deg)\n",
    "            sun_to_asteroid_values.append(f'{round(np.mean(asteroid_ephem[\"r\"]),4)}')                 # Heliocentric Distance of the Asteroid (AU)\n",
    "            asteroid_to_jwst_values.append(f'{round(np.mean(asteroid_ephem[\"delta\"]),4)}')            # Distance from the Asteroid to JWST (AU) \n",
    "            position_uncertainty_values.append(f'{round(np.mean(asteroid_ephem[\"RSS_3sigma\"]),4)}')   # Root-Sum-Square of 3-Sigma Uncertainty\n",
    "            \n",
    "            midpoint = len(asteroid_ephem['RA']) // 2\n",
    "            position_values.append([asteroid_ephem['RA'][midpoint], asteroid_ephem['DEC'][midpoint]])\n",
    "            position_Rates.append([asteroid_ephem['RA_rate'][midpoint], asteroid_ephem['DEC_rate'][midpoint]])\n",
    "            Visual_Mag.append(str(asteroid_ephem['V'][midpoint]))\n",
    "            Surface_Brightnesses.append(str(asteroid_ephem['surfbright'][midpoint]) if '<NA>' not in str(asteroid_ephem['surfbright'][midpoint]) else '-')\n",
    "            \n",
    "            motion =np.sqrt(float(asteroid_ephem['RA_rate'][midpoint])**2 + float(asteroid_ephem['DEC_rate'][midpoint])**2) * (XPOSURE/60)/60\n",
    "            distance_arcsec_over_exptime.append(f'{round(motion, 4) if not isinstance(motion, str) else motion}')\n",
    "            \n",
    "            \n",
    "        \n",
    "        for indx in range(len(contained_asteroid_names)):\n",
    "            asteroid_name = contained_asteroid_names[indx]\n",
    "            classification, radius, albedo, short_name = asteroid_physical(asteroid_name)\n",
    "            asteroid_class_list.append(classification)\n",
    "            asteroid_short_name_list.append(short_name)\n",
    "            asteroid_radius_list.append(radius)\n",
    "            asteroid_albedo_list.append(albedo)\n",
    "        \n",
    "            Image_parameters = [BUNIT, PIXAR_SR, PIXAR_A2]\n",
    "        \n",
    "            # Find the center of the asteroid\n",
    "            asteroid_center_crop, asteroid_center_original, streak_distance_in_pix, center_offset_sr, streakFlag = find_asteroid_center(imageData, Image_parameters, containedAsteroidX[indx], \n",
    "                                                                                                                           containedAsteroidY[indx], float(position_uncertainty_values[indx]), filter_name)  \n",
    "            #Run Flux Calculation\n",
    "            aperture_flux, flux_error, annulus_median_ratio, signal2noise, cropped_image, pixel_offsets, ap_NAN_percent, ann_NAN_percent = Flux_Calculation(filter_name, Image_parameters, imageData, asteroid_center_original, \n",
    "                                                                                                               asteroid_name, observation, XPOSURE, folder_path, produce_images)\n",
    "            \n",
    "            adjusted_asteroid_X = containedAsteroidX[indx] - pixel_offsets[0]\n",
    "            adjusted_asteroid_Y = containedAsteroidY[indx] - pixel_offsets[1]\n",
    "            \n",
    "            ann_nans.append(str(round(ann_NAN_percent,2)))\n",
    "            ap_nans.append(str(round(ap_NAN_percent,2)))\n",
    "            \n",
    "            distance_pix.append(f'{round(streak_distance_in_pix, 4) if not isinstance(streak_distance_in_pix, str) else streak_distance_in_pix}')\n",
    "                \n",
    "            streak_flags.append(streakFlag)\n",
    "            \n",
    "            aperture_flux_mJy_list.append(aperture_flux)\n",
    "            flux_error_list.append(flux_error)\n",
    "            annulus_median_ratio_list.append(f'{round(annulus_median_ratio,4)}') \n",
    "            signal_to_noise_list.append(f'{round(signal2noise,2)}')\n",
    "            center_offset_sr_list.append(f'{round(center_offset_sr*1e13,4)}')\n",
    "            \n",
    "            if produce_images:\n",
    "                #highContrastImage(cropped_image, asteroid_name, folder_path, observation)\n",
    "                highContrastImageWithOverlay(cropped_image, asteroid_name, adjusted_asteroid_X, adjusted_asteroid_Y, folder_path, observation)\n",
    "        \n",
    "            \n",
    "        if produce_images:\n",
    "            produceOverlayImage( imageData, WCS, observation, contained_asteroid_names, containedAsteroidX, containedAsteroidY, folder_path)\n",
    "            #produceOriginalImage(imageData, WCS, observation, folder_path)\n",
    "\n",
    "        \n",
    "        results = [(round(XPOSURE/60,4)), combine_strings(contained_asteroid_names), combine_strings(asteroid_short_name_list), combine_strings(asteroid_class_list), combine_strings(asteroid_radius_list),\n",
    "                    combine_strings(asteroid_albedo_list), combine_strings(alpha_values), combine_strings(sun_to_asteroid_values), combine_strings(asteroid_to_jwst_values),\n",
    "                    ', '.join(f'({x}, {y})' for x, y in position_values), ', '.join(f'({x}, {y})' for x, y in position_Rates), combine_strings(position_uncertainty_values),READOUT, SUBARRAY, BITPIX, DITHERTYPE, BRIGHTSKY, \n",
    "                    BUNIT, f'{round(PHOTMJSR,4)}', float(f\"{PIXAR_A2:.4g}\"), combine_strings(Visual_Mag), combine_strings(Surface_Brightnesses), combine_strings(aperture_flux_mJy_list), combine_strings(flux_error_list), combine_strings(annulus_median_ratio_list), \n",
    "                   combine_strings(signal_to_noise_list), combine_strings(distance_pix), combine_strings(distance_arcsec_over_exptime), combine_strings(center_offset_sr_list), combine_strings(ap_nans), combine_strings(ann_nans),\n",
    "                   combine_strings(streak_flags)]\n",
    "\n",
    "        return(results)       \n",
    "\n",
    "    else:\n",
    "        return(['' for _ in range(24)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e9e6e7f0-0f1d-4804-b196-cf29a7f617db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def level2_asteroid_search(row, Proposal_Asteroid_Dict, produce_images, folderName):\n",
    "    #Begin the asteroid search process \n",
    "    \n",
    "    proposal =      row['proposal_id']\n",
    "    observation =   row['observationid']\n",
    "    polygonString = row['position_bounds_spoly']\n",
    "    expStartMJD =   row['time_bounds_lower']\n",
    "    expEndMJD =     row['time_bounds_upper']\n",
    "    filter_name =   row['energy_bandpassname']\n",
    "    \n",
    "    Asteroid_List = Proposal_Asteroid_Dict[proposal]\n",
    "    \n",
    "    #convert the format of the polygon string\n",
    "    poly_string_formatted = formatPolygon(str(polygonString))\n",
    "    \n",
    "    #Convert Exp Time to proper Format\n",
    "    expStart = (MJDconversion(expStartMJD))\n",
    "    expEnd   = (MJDconversion(expEndMJD))\n",
    "        \n",
    "    #path\n",
    "    propPath = f'{folderName}/{proposal}'\n",
    "        \n",
    "    results = level2_asteroid_names(expStart, expEnd, filter_name, poly_string_formatted, proposal, observation, Asteroid_List, propPath, produce_images)\n",
    "\n",
    "    return(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe7eb3d-ad9d-4329-831b-b03316c01976",
   "metadata": {
    "tags": []
   },
   "source": [
    "## On Detector Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa2a052f-8f72-402f-a3ed-9f6c592fa8ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def onDetectorCheck(imageData, X, Y):\n",
    "    # Ensures that there is measurable data from the pixels corresponding to the JPL asteroid positions\n",
    "    \n",
    "    # Convert coordinates to integers (floor them)\n",
    "    X_int = np.floor(X).astype(int)\n",
    "    Y_int = np.floor(Y).astype(int)\n",
    "\n",
    "    # Find which coordinates are within the bounds of the image\n",
    "    valid_coords = (X_int >= 0) & (X_int < imageData.shape[1]) & (Y_int >= 0) & (Y_int < imageData.shape[0])\n",
    "\n",
    "    # Get pixel values at the valid coordinates\n",
    "    probe_values = imageData[Y_int[valid_coords], X_int[valid_coords]]\n",
    "\n",
    "    # Check if any of the valid pixel values are non-zero and not NaN\n",
    "    if np.any((probe_values != 0) & ~np.isnan(probe_values)):\n",
    "        return True\n",
    "\n",
    "    # Additional checks at a distance of 3 pixels\n",
    "    offsets = [(5,5), (5,-5), (-5,5), (-5,-5)]  # +5/-5 in X and Y directions\n",
    "    for dx, dy in offsets:\n",
    "        # Adjust coordinates\n",
    "        X_offset = X_int + dx\n",
    "        Y_offset = Y_int + dy\n",
    "\n",
    "        # Ensure the new coordinates are within bounds\n",
    "        valid_coords_offset = (X_offset >= 0) & (X_offset < imageData.shape[1]) & (Y_offset >= 0) & (Y_offset < imageData.shape[0])\n",
    "\n",
    "        # Get pixel values for valid offset coordinates\n",
    "        offset_values = imageData[Y_offset[valid_coords_offset], X_offset[valid_coords_offset]]\n",
    "\n",
    "        # Check if any of the offset pixel values are non-zero and not NaN\n",
    "        if np.any((offset_values != 0) & ~np.isnan(offset_values)):\n",
    "            return True\n",
    "\n",
    "    # If no valid pixel values are found in the initial or offset tests\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f727cfa-611f-4773-b074-35b9b9c75172",
   "metadata": {},
   "source": [
    "## Retrieve Asteroid Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "38d5246e-0967-4b40-93ab-23add738fc07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def asteroid_physical(asteroid_name):  \n",
    "    # Pull the ASteroids physical parameters from the online api\n",
    "    \n",
    "    # Default values for radius and albedo\n",
    "    # NOTE: these are not always documented on JPL Horizons\n",
    "    radius = '-'\n",
    "    albedo = '-'\n",
    "    classification = 'NA'\n",
    "    short_name = '-'\n",
    "    \n",
    "    # Retrieve asteroid classification\n",
    "    try:\n",
    "        # Make the request to the API\n",
    "        url = \"https://ssd-api.jpl.nasa.gov/sbdb.api\"\n",
    "        data = {'sstr': asteroid_name}\n",
    "        response = requests.get(url, params=data)\n",
    "\n",
    "        # Parse the JSON string into a Python dictionary\n",
    "        parsed_data = json.loads(response.text)\n",
    "\n",
    "        # Extract the classification term\n",
    "        classification = parsed_data[\"object\"][\"orbit_class\"][\"name\"]\n",
    "        if \"shortname\" in parsed_data[\"object\"]:\n",
    "            short_name = parsed_data[\"object\"][\"shortname\"]\n",
    "        \n",
    "    except:\n",
    "        logging.info(f\"Could not pull api information for {asteroid_name}\")\n",
    "        pass\n",
    "\n",
    "    # Retrieve asteroid physical parameters (if they are known)\n",
    "    # NOTE: at this time the method for obtaining this is not ideal as it uses an error readout \n",
    "    # it is not known how to retrieve this data from the horizons output otherwise\n",
    "    try:\n",
    "        jpl_output = Horizons(id=asteroid_name, location='500')\n",
    "        element = jpl_output.elements().items()\n",
    "    except Exception as e:\n",
    "        error_message= str(e)\n",
    "    # Search for RAD and ALBEDO in the string\n",
    "    radius_match = re.search(r'RAD=\\s*([0-9.]+)', error_message)\n",
    "    albedo_match = re.search(r'ALBEDO=\\s*([0-9.]+)', error_message)\n",
    "    \n",
    "    # If found, assign the values to radius and albedo\n",
    "    if radius_match:\n",
    "        radius = radius_match.group(1)\n",
    "    if albedo_match:\n",
    "        albedo = albedo_match.group(1)\n",
    "    \n",
    "    return (classification, radius, albedo, short_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47af76d2-7298-4b8d-80c0-a5b5943baf31",
   "metadata": {},
   "source": [
    "## Detecting The Asteroid Center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da214cdf-d1b5-4517-9bde-8e1daf7795d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_pixel_value(array, pixels):\n",
    "    # Return statistics on the pixel values for the corresponding coordinates\n",
    "    pixel_values = np.array(array)[pixels[:, 0], pixels[:, 1]]\n",
    "    valid_pixel_values = pixel_values[~np.isnan(pixel_values)]\n",
    "    \n",
    "    # Count NaN values\n",
    "    total_pixels = len(pixel_values)\n",
    "    nan_count = np.isnan(pixel_values).sum()\n",
    "    nan_percentage = (nan_count / total_pixels) * 100 if total_pixels > 0 else 100\n",
    "    \n",
    "    total = np.sum(valid_pixel_values)\n",
    "    amount = len(valid_pixel_values)\n",
    "    average = total / amount if amount > 0 else float('nan')\n",
    "    \n",
    "    return total, amount, average, nan_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fd69983-95cd-4c49-936b-7458a509bf3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_pixels_within_radius(array_size, center, radius):\n",
    "    # Determine which pixels are within the provided distance from some central target\n",
    "    \n",
    "    # Adjust radius to remove partially enclosed pixels (looking for pixels that are fully contained\n",
    "    # NOTE: these measurments are from the center of the pixel \n",
    "    adjusted_radius = radius - 0.4 #pixels\n",
    "    \n",
    "    x, y = np.meshgrid(np.arange(array_size[1]), np.arange(array_size[0]))\n",
    "    distances = np.sqrt((x - center[0])**2 + (y - center[1])**2)\n",
    "    pixels_within_radius = np.column_stack(np.where(distances <= adjusted_radius))\n",
    "    \n",
    "    return pixels_within_radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "029d067a-743c-497f-a296-24d5feb284d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def locate_bright_region(array, desired_percentile = 98):\n",
    "    # Determine the average location of pixels within the array that are within the top 98% of pixel values\n",
    "    \n",
    "    flattened_array = array.flatten()\n",
    "\n",
    "    # Determine bright pixels\n",
    "    custom_percentile_value = np.nanpercentile(flattened_array, desired_percentile)\n",
    "\n",
    "    # Create a mask where values are above the custom percentile, ignoring NaNs\n",
    "    mask = array >= custom_percentile_value\n",
    "\n",
    "    # Find the coordinates of the thresholded values\n",
    "    y, x = np.where(mask)\n",
    "\n",
    "    # Calculate the mean coordinates\n",
    "    xmean = np.mean(x)\n",
    "    ymean = np.mean(y)\n",
    "    \n",
    "    return([xmean, ymean])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4cff6f3d-e0da-465f-a71a-796b3ee37265",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def replace_nan_edges(image):\n",
    "    # Replace all nan value pixels within an image crop to the average of the image\n",
    "    # NOTE: this is only used for determining the center of the asteroid\n",
    "\n",
    "    modified_image = image.copy()\n",
    "    \n",
    "    # Create a boolean mask of NaN values\n",
    "    nan_mask = np.isnan(image)\n",
    "    \n",
    "    # Create a mask for border-connected NaNs\n",
    "    border_mask = np.zeros_like(image, dtype=bool)\n",
    "    \n",
    "    # Set border-connected NaNs to True\n",
    "    border_mask[:, 0]  = nan_mask[:, 0]   # Left border\n",
    "    border_mask[:, -1] = nan_mask[:, -1]  # Right border\n",
    "    border_mask[0, :]  = nan_mask[0, :]   # Top border\n",
    "    border_mask[-1, :] = nan_mask[-1, :]  # Bottom border\n",
    "    \n",
    "    # Propagate the border-connected mask to all connected NaN regions, using labels\n",
    "    labeled, num_features = label(nan_mask)  \n",
    "    border_connected_labels = np.unique(labeled[border_mask])  \n",
    "    \n",
    "    # Create a mask for border-connected NaNs\n",
    "    border_connected_nans = np.isin(labeled, border_connected_labels)\n",
    "    \n",
    "    # Replace these NaNs with average pixel in the copied image\n",
    "    modified_image[border_connected_nans] = np.nanmean(image)\n",
    "    \n",
    "    return modified_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f2ea9693-c812-4125-b3e0-754397f66ff5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def refine_center(array, initial_center, max_iter=50, radius=20, min_radius=4, convergence_threshold=1e-3, thresh_perc = 80):\n",
    "   \n",
    "    center = np.array(initial_center)\n",
    "\n",
    "    for iteration in range(max_iter):\n",
    "        # Restrict search to a circular region\n",
    "        x, y = np.meshgrid(np.arange(array.shape[1]), np.arange(array.shape[0]))\n",
    "\n",
    "        # Find the distance between all pixels and the center\n",
    "        distances = np.sqrt((x - center[0])**2 + (y - center[1])**2)\n",
    "\n",
    "        # Dynamically adjust the mask size based on iteration\n",
    "        current_radius = max(radius * (1 - iteration / max_iter), min_radius)\n",
    "        \n",
    "        # Only interested in pixels within the radius\n",
    "        mask_dist = (distances <= current_radius)\n",
    "        \n",
    "        # Handle NaNs: Replace NaN values in the array with 0 for calculations\n",
    "        masked_array_dist = np.nan_to_num(array[mask_dist], nan=0)\n",
    "        \n",
    "        # Only interested in the brightest 80% of pixels\n",
    "        flattened_array = masked_array_dist.flatten()\n",
    "\n",
    "        # Determine bright pixels\n",
    "        intensity_threshold = np.nanpercentile(flattened_array, thresh_perc)\n",
    "    \n",
    "        # Create a mask for pixels within the radius and above the intensity threshold\n",
    "        mask = (distances <= current_radius) & (array >= intensity_threshold)\n",
    "\n",
    "        # Handle NaNs: Replace NaN values in the array with 0 for calculations\n",
    "        masked_array = np.nan_to_num(array[mask], nan=0)\n",
    "\n",
    "        # Recompute weighted centroid within this region\n",
    "        total_intensity = np.sum(masked_array)\n",
    "\n",
    "        if total_intensity == 0:  # To avoid division by zero\n",
    "            #Intensity == 0 at iteration {iteration}. Exiting.\n",
    "            break\n",
    "\n",
    "        x_center = np.sum(x[mask] * masked_array) / total_intensity\n",
    "        y_center = np.sum(y[mask] * masked_array) / total_intensity\n",
    "\n",
    "        new_center = np.array([x_center, y_center])\n",
    "\n",
    "        # Check for convergence\n",
    "        if np.allclose(new_center, center, atol=convergence_threshold * (iteration + 1) / max_iter):\n",
    "            #Converged at iteration\n",
    "            break\n",
    "\n",
    "        # Update the center for the next iteration\n",
    "        center = new_center\n",
    "\n",
    "    return center"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5e8796-3826-4121-b059-a249a1f4a690",
   "metadata": {},
   "source": [
    "## Determine Asteroid Flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "29989249-3f71-4fd8-bcc1-5462363e4154",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Flux_Calculation(image_filter, image_parameters, image_data, center, asteroid_name, observation, exposureTimeSeconds, folder_path, produce_image, extra_pixels = 15):\n",
    "    # Crop the image near the asteroid\n",
    "\n",
    "    filters_psf = { \"F560W\":  [3.87,  10.78, 18.39, 1.436],\n",
    "                    \"F770W\":  [4.22,  8.92,  14.64, 1.442],\n",
    "                    \"F1000W\": [4.60,  6.70,  11.58, 1.453],\n",
    "                    \"F1130W\": [4.92,  7.06,  11.22, 1.465],\n",
    "                    \"F1280W\": [5.18,  7.61,  10.99, 1.483],\n",
    "                    \"F1500W\": [5.69,  8.63,  11.45, 1.497],\n",
    "                    \"F1800W\": [6.29,  10.09, 12.52, 1.506],\n",
    "                    \"F2100W\": [7.91,  11.90, 16.07, 1.492],\n",
    "                    \"F2550W\": [9.18,  14.03, 17.71, 1.506],\n",
    "                    \"FND\":    [5.27,  8.98,  13.74, 1.455],\n",
    "                    \"F1065C\": [10.07, 17.77, 24.80, 1.464],\n",
    "                    \"F1140C\": [10.68, 19.10, 27.06, 1.461],\n",
    "                    \"F1550C\": [14.21, 25.52, 36.77, 1.459],\n",
    "                    \"F2300C\": [13.27, 16.84, 27.25, 1.470]}\n",
    "    \n",
    "    miri_sensativity = {\"F560W\":  0.13, #microJansky\n",
    "                        \"F770W\":  0.24,\n",
    "                        \"F1000W\": 0.46,\n",
    "                        \"F1130W\": 1.02,\n",
    "                        \"F1280W\": 0.83,\n",
    "                        \"F1500W\": 1.18,\n",
    "                        \"F1800W\": 2.42,\n",
    "                        \"F2100W\": 4.70,\n",
    "                        \"F2550W\": 15.3,}\n",
    "\n",
    "    apertureRange, innerAnnulus, outerAnnulus, correctionFactor = filters_psf[image_filter]\n",
    "    \n",
    "    rows, cols = image_data.shape\n",
    "\n",
    "    # Calculate crop boundaries with bounds checking using numpy.clip and additional buffer pixels\n",
    "    col_start, col_end = np.clip([int(center[0] - outerAnnulus - extra_pixels), int(center[0] + outerAnnulus + extra_pixels)], 0, cols - 1)\n",
    "    row_start, row_end = np.clip([int(center[1] - outerAnnulus - extra_pixels), int(center[1] + outerAnnulus + extra_pixels)], 0, rows - 1)\n",
    "\n",
    "    # Extract cropped region\n",
    "    cropped_image = image_data[row_start:row_end, col_start:col_end]\n",
    "    \n",
    "    center_crop = np.array([center[0] - col_start, center[1] - row_start])\n",
    "\n",
    "    current_pixel_units, sr_to_pix_conversion, arcsec_to_pix_conversion = image_parameters\n",
    "    \n",
    "    image_shape = cropped_image.shape\n",
    "\n",
    "    #add 0.4 to the inner annulus radius to remove partial pixels\n",
    "    pixels_in_aperture =      get_pixels_within_radius(image_shape, center_crop, apertureRange)\n",
    "    pixels_in_inner_annulus = get_pixels_within_radius(image_shape, center_crop, innerAnnulus + 0.4)\n",
    "    pixels_in_outer_annulus = get_pixels_within_radius(image_shape, center_crop, outerAnnulus)\n",
    "    \n",
    "    #pixels_in_annulus = [unique for unique in pixels_in_outer_annulus if not is_in_array(unique, pixels_in_inner_annulus)]\n",
    "    pixels_in_annulus = np.array([unique for unique in pixels_in_outer_annulus if not is_in_array(unique, pixels_in_inner_annulus)])\n",
    "    \n",
    "    pixel_aperture_sumtotal, pixel_aperture_count, pixel_aperture_average, ap_NAN_percent  = get_pixel_value(cropped_image, pixels_in_aperture)\n",
    "    pixel_annulus_sumtotal,  pixel_annulus_count,  pixel_annulus_average, ann_NAN_percent  = get_pixel_value(cropped_image, pixels_in_annulus)\n",
    "    \n",
    "    #Determine the average background value per pixel\n",
    "    anulus_average_MJy = (float(pixel_annulus_sumtotal) * float(sr_to_pix_conversion)) / float(pixel_annulus_count)\n",
    "    \n",
    "    #subtract the backgound amount (background/pixel * aperture pixel amount) from the total aperture amount\n",
    "    aperture_MJy = (float(pixel_aperture_sumtotal) * float(sr_to_pix_conversion)) - (float(anulus_average_MJy) * float(pixel_aperture_count))\n",
    "    \n",
    "    #Convert the units from MJy to mJy since they are faint (liekly)\n",
    "    aperture_mJy =  aperture_MJy * 1e9\n",
    "\n",
    "    annulus_average = pixel_annulus_average * float(sr_to_pix_conversion) * 1e9\n",
    "    image_median = np.nanmedian(cropped_image) * float(sr_to_pix_conversion) * 1e9\n",
    "    \n",
    "    \n",
    "    if pixel_annulus_count == 0 or pixel_annulus_sumtotal == 0 or pixel_aperture_count == 0:\n",
    "        signal_to_noise_ratio = 0\n",
    "    else:\n",
    "        if image_filter in miri_sensativity:\n",
    "            flux = aperture_mJy\n",
    "            exposureTime = exposureTimeSeconds\n",
    "            miriSensativity = miri_sensativity[image_filter]\n",
    "            signal_to_noise_ratio = 10*((flux*1000)/miriSensativity)*np.sqrt(exposureTime/10000)\n",
    "                        \n",
    "        else:                     \n",
    "            noise_in_aperture = (float(anulus_average_MJy) * float(pixel_aperture_count))\n",
    "            sky_subtracted_source_signal = (float(pixel_aperture_sumtotal) * float(sr_to_pix_conversion)) - noise_in_aperture\n",
    "            signal_to_noise_ratio = sky_subtracted_source_signal / noise_in_aperture\n",
    "    \n",
    "    if produce_image:\n",
    "        # Visualization\n",
    "        fig, ax = plt.subplots()\n",
    "        # Array with Estimated Center\n",
    "        cax = ax.imshow(cropped_image)\n",
    "        ax.scatter([center_crop[0]], [center_crop[1]], c='white', s=30, label='Estimated Center')\n",
    "\n",
    "        circle1 = Circle((center_crop[0], center_crop[1]), apertureRange, edgecolor='tab:green', facecolor='none', linewidth=4, label=f\"Aperture\")\n",
    "        circle2 = Circle((center_crop[0], center_crop[1]), innerAnnulus,  edgecolor='tab:red',   facecolor='none', linewidth=4, label=f\"Annulus\")\n",
    "        circle3 = Circle((center_crop[0], center_crop[1]), outerAnnulus,  edgecolor='tab:red',   facecolor='none', linewidth=4)\n",
    "\n",
    "        # Add the circle to the plot\n",
    "        ax.add_patch(circle1)\n",
    "        ax.add_patch(circle2)\n",
    "        ax.add_patch(circle3)\n",
    "\n",
    "        ax.scatter([], [], c='green', marker = 's')\n",
    "        ax.scatter([], [], c='red', marker = 's')\n",
    "        ax.set_title(f\"Asteroid {asteroid_name}, Filter: {image_filter}, Flux: {round(aperture_mJy * correctionFactor,4)}mJy, SNR: {round(signal_to_noise_ratio,2)}\")\n",
    "        ax.legend()\n",
    "        plt.gca().invert_yaxis()\n",
    "\n",
    "        cbar = plt.colorbar(cax)\n",
    "        cbar.set_label(f'Intensity ({current_pixel_units})', rotation=270, labelpad=15)\n",
    "        \n",
    "        plt.savefig(f\"{folder_path}/FluxRegion_{observation}_{asteroid_name}.png\", bbox_inches='tight')\n",
    "        #plt.show()\n",
    "        plt.close()\n",
    "\n",
    "    \n",
    "    flux_output = f\"{round(aperture_mJy * correctionFactor,4)}\"\n",
    "    \n",
    "    if signal_to_noise_ratio == 0:\n",
    "        flux_error = \"100.0\"\n",
    "    elif signal_to_noise_ratio >= 20:\n",
    "        flux_error = \"5.0\"\n",
    "    else:\n",
    "        flux_error = str(round(100 / float(signal_to_noise_ratio), 1)) \n",
    "    \n",
    "    return(flux_output, flux_error, annulus_average/image_median, signal_to_noise_ratio, cropped_image, [col_start, row_start], ap_NAN_percent, ann_NAN_percent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bb7f7e-2895-4518-9c8d-2ccd944d8717",
   "metadata": {},
   "source": [
    "## Output Image Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c02baf12-eab9-4d48-bc6b-de8c3018f1b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def produceOriginalImage(imageData, WCS, obsID, filePath):\n",
    "    # Produce and save the original full observation image\n",
    "    \n",
    "    output_file_path = f'{filePath}/Original_{obsID}.png'\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 10), subplot_kw={'projection': WCS})\n",
    "    ax.set_xlabel('RA')\n",
    "    ax.set_ylabel('DEC')\n",
    "    ax.set_title(f'{obsID},  i2d Image')\n",
    "\n",
    "    # Normalize image data for better presentation\n",
    "    norm = simple_norm(imageData, 'sqrt')\n",
    "    ax.imshow(imageData, cmap='viridis', norm=norm)\n",
    "\n",
    "    plt.savefig(output_file_path, bbox_inches='tight')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f3b83b0-1d7b-462e-8dac-6518bd2d161c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def produceOverlayImage(imageData, WCS, obsID, asteroidList, Xlist, Ylist, filePath):\n",
    "    # Produce and save the full observation image with identification region for known asteroids, and cutout subplots\n",
    "    \n",
    "    output_file_path = f'{filePath}/Overlay_{obsID}.png'\n",
    "    \n",
    "    # Parameters for subplot size\n",
    "    subplot_width = 7  # Width of each subplot\n",
    "    subplot_height = 8  # Height of each subplot\n",
    "    n_subplots = len(asteroidList) + 1  # Number of subplots\n",
    "\n",
    "    # Calculate the total figure size\n",
    "    fig_width = subplot_width * n_subplots\n",
    "    fig_height = subplot_height\n",
    "\n",
    "    \n",
    "    # Set figure size based on the number of asteroids, each getting their own subplot\n",
    "    fig, axes = plt.subplots(1, n_subplots, figsize=(fig_width, fig_height), subplot_kw={'projection': WCS})\n",
    "    \n",
    "    # First subplot is for the full observation image with bounding box around asteroid\n",
    "    ax1 = axes[0]  \n",
    "    ax1.set_xlabel('RA')\n",
    "    ax1.set_ylabel('DEC')\n",
    "    ax1.set_title(f'{obsID},  i2d Image')\n",
    "\n",
    "    # Normalize image data for better presentation\n",
    "    norm = simple_norm(imageData, 'sqrt')\n",
    "    ax1.imshow(imageData, cmap='viridis', norm=norm, aspect='auto', zorder=1)\n",
    "\n",
    "    # Define asteroid-specific colors for plotting, these must contrast the background well (max = 13 asteorids in an image)\n",
    "    colors = ['orangered', 'maroon', 'firebrick', 'goldenrod', 'tomato', 'chocolate', \n",
    "              'tab:red', 'tab:orange', 'red', 'orange',  'black', 'yellow', 'blueviolet']\n",
    "\n",
    "    # Loop through asteroids and overlay positions, each getting their own subplot cutout and overlay\n",
    "    for indx, asteroid in enumerate(asteroidList):\n",
    "        ax_ = axes[indx + 1] \n",
    "        ax_.set_xlabel('RA')\n",
    "        ax_.set_ylabel('DEC')\n",
    "        ax_.set_title(f'{obsID} Asteroid {asteroid}')\n",
    "        \n",
    "\n",
    "        # Define X and Y positions of the enclosed Asteroids\n",
    "        X = Xlist[indx]\n",
    "        Y = Ylist[indx]\n",
    "        \n",
    "        # Add buffer space around the asteroid position\n",
    "        extraPixels = 20  \n",
    "\n",
    "        # Get positions of the asteroid bounds for crop region\n",
    "        if len(X) > 1:\n",
    "            P1 = [X[0], Y[0]]\n",
    "            P2 = [X[-1], Y[-1]]\n",
    "        else:\n",
    "            P1 = [X[0], Y[0]]\n",
    "            P2 = [X[0], Y[0]]\n",
    "\n",
    "        # Define the default bounds with extra space \n",
    "        lowerx = round(min(P1[0], P2[0])) - extraPixels\n",
    "        upperx = round(max(P1[0], P2[0])) + extraPixels\n",
    "        lowery = round(min(P1[1], P2[1])) - extraPixels\n",
    "        uppery = round(max(P1[1], P2[1])) + extraPixels\n",
    "        \n",
    "        rows, cols = imageData.shape\n",
    "\n",
    "        # Calculate crop boundaries with bounds checking using numpy.clip and additional buffer pixels\n",
    "        col_start, col_end = np.clip([lowerx, upperx], 0, cols - 1)\n",
    "        row_start, row_end = np.clip([lowery, uppery], 0, rows - 1)\n",
    "        \n",
    "        # Extract cropped region\n",
    "        cropped_image = imageData[row_start:row_end, col_start:col_end]\n",
    "        \n",
    "        # Normalize image data for better presentation\n",
    "        ax_.imshow(cropped_image, cmap='viridis', zorder=1)\n",
    "\n",
    "        # Plot the bounding box and asteroid positions\n",
    "        ax1.plot([lowerx, upperx, upperx, lowerx, lowerx], \n",
    "                 [lowery, lowery, uppery, uppery, lowery], \n",
    "                 alpha=0.7, c=colors[indx], linewidth=2, zorder=2, label=asteroid)\n",
    "        \n",
    "        ax_.scatter(X - col_start, Y - row_start, s=15, alpha=0.8, c=colors[indx], zorder=2, label='JPL Coordinates')\n",
    "        #ax_.set_xlim(lowerx, upperx)\n",
    "        #ax_.set_ylim(lowery, uppery)\n",
    "        ax_.legend()\n",
    "\n",
    "    ax1.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(output_file_path, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "34ddd124-b0ee-4a41-956e-20cfb063c534",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def highContrastImage(cropped_image, asteroidName, filePath, observation):\n",
    "    # Plot the high-contrast zoomed image to assist in asteroid visual identification\n",
    "    \n",
    "    output_file_path = f'{filePath}/HighContrastZoom_{observation}_{asteroidName}.png'\n",
    "    \n",
    "    plt.figure(figsize=(12, 12)) \n",
    "    plt.title(f\"{observation},  Cutout of Asteroid {asteroidName}\")\n",
    "    plt.xlabel('Pixel Columns')\n",
    "    plt.ylabel('Pixel Rows')\n",
    "\n",
    "    # Normalize image data for better presentation\n",
    "    cax = plt.imshow(cropped_image, cmap='viridis')\n",
    "    plt.colorbar(cax)\n",
    "    plt.gca().invert_yaxis()\n",
    "\n",
    "    plt.savefig(output_file_path, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2955d409-3ea8-4fa7-af6e-5db7b5b7a357",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def highContrastImageWithOverlay(cropped_image, asteroidName, X,Y, filePath, observation):\n",
    "    # Plot the high-contrast zoomed image to assist in asteroid visual identification, with asteroid overlay\n",
    "    \n",
    "    output_file_path = f'{filePath}/HighContrastZoomOverlay_{observation}_{asteroidName}.png'\n",
    "    \n",
    "    plt.figure(figsize=(12, 12)) \n",
    "    plt.title(f\"{observation},  Cutout of Asteroid {asteroidName}\")\n",
    "    plt.xlabel('Pixel Columns')\n",
    "    plt.ylabel('Pixel Rows')\n",
    "\n",
    "    # Normalize image data for better presentation\n",
    "    cax = plt.imshow(cropped_image, cmap='viridis')\n",
    "    \n",
    "    # Overlay the Asteroid probe points (JPL), and a directional arrow for multiple points\n",
    "    if len(X) > 1:\n",
    "        \n",
    "        direction_x, direction_y = X[-1] - X[0], Y[-1] - Y[0]\n",
    "        distance = np.hypot(direction_x, direction_y)\n",
    "\n",
    "        unit_x, unit_y = direction_x / distance, direction_y / distance\n",
    "        start_x, start_y = X[0] - unit_x * distance * 0.4, Y[0]  - unit_y * distance * 0.4\n",
    "        end_x, end_y = X[-1] + unit_x * distance * 0.7, Y[-1] + unit_y * distance * 0.7\n",
    "\n",
    "        # Add JPL probe coordinates \n",
    "        plt.scatter(X, Y, color='orange', alpha=0.7, label='JPL Coordinates')\n",
    "        \n",
    "        # Draw a faint arrow pointing through the data points\n",
    "        plt.arrow(start_x, start_y, end_x - start_x, end_y - start_y,\n",
    "                  color='orange', alpha=0.3, width=0.8, head_width=1.5, head_length=1.6)\n",
    "        \n",
    "    else:\n",
    "        # Plot only the JPL Coordinate\n",
    "        plt.scatter(X, Y, color='orange', alpha=0.5, label='JPL Coordinates')\n",
    "       \n",
    "    plt.colorbar(cax)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.legend()\n",
    "\n",
    "    plt.savefig(output_file_path, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276d0328-0e04-4208-8680-89f3cd70efbb",
   "metadata": {},
   "source": [
    "## Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4fb54a9b-4768-4f01-8e11-c36276d62c93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main(propRange, instrumentName, dataType, volume, additionalFilters=[], folderName = 'Results', level3_CSV_Input_Path = '', produce_image = True):\n",
    "    \n",
    "    \"\"\"\n",
    "    OUTPUTS:\n",
    "    Level 3:\n",
    "            2 Level 3 Asteroid Detection CSV \n",
    "                    1, Full Documentation on all observations\n",
    "                    2, Documentation on ONLY Observations containing Known-Asteroids\n",
    "    \n",
    "    Level 2:\n",
    "            2 Level 2 Asteroid Detection CSV \n",
    "                    1, Full Documentation on all observations\n",
    "                    2, Documentation on ONLY Observations containing Known-Asteroids\n",
    "                    \n",
    "            Folders named after proposals containing Known-Asteroids\n",
    "                    Plot of Observation\n",
    "                    Plot of Observation with Known-Asteroid Overlay\n",
    "                    \n",
    "                    Plot of Observation Cropped around Known-Asteroid\n",
    "                    Plot of Observation Cropped around Known-Asteroid with Overlay\n",
    "                    \n",
    "                    Plot of Flux Region Determined (Aperture and Annulus)\n",
    "    \"\"\" \n",
    "    \n",
    "    # Main function to run the JWST Known-Asteroid Detection\n",
    "    if '/' in level3_CSV_Input_Path:\n",
    "        folder_path = os.path.dirname(level3_CSV_Input_Path)\n",
    "    else:\n",
    "        folder_path = './'\n",
    "\n",
    "    current_time = datetime.now()\n",
    "    timestamp = current_time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    log_filename = f\"{folder_path}/Asteroid_Detection_log_{timestamp}.txt\"\n",
    "\n",
    "    # Configure logging to write to a file\n",
    "    logging.basicConfig(\n",
    "        filename=log_filename,  # Log file\n",
    "        level=logging.INFO,     # Logging level\n",
    "        force=True,             #prevent overwriting data\n",
    "        format=\"%(message)s\"    # Log message format\n",
    "    )\n",
    "\n",
    "    # Log messages (these will go to the file only)\n",
    "    logging.info(f\"This run was performed at {timestamp}\\n\")\n",
    "    \n",
    "    # Generate the output folder for the results\n",
    "    generateFolder(folderName)           \n",
    "            \n",
    "    ##### ----- LEVEL 3 ----- #####   \n",
    "    \n",
    "    # If the Level 3 CSV has already been generated then dont rerun it\n",
    "    if level3_CSV_Input_Path == '':\n",
    "        print('Performing Level 3 Search')\n",
    "        start_time = time.time()\n",
    "        logging.info('Performing Level 3 Search')\n",
    "        # Perform an archive search for level 3 observations meeting the qualifications defined in the main function\n",
    "        Level_3_Archive_DF = level3_Archive_Query(propRange, instrumentName, dataType, volume, additionalFilters)\n",
    "        \n",
    "        if Level_3_Archive_DF.empty:\n",
    "            print(f\"Archive does not contain observations in the range {propRange}\")\n",
    "            logging.info(f'Archive does not contain observations in the range {propRange}')\n",
    "            return\n",
    "        \n",
    "        # Itterate through the level 3 archive data and check images for asteroids\n",
    "        Level_3_Archive_DF['Asteroids'] = Level_3_Archive_DF.progress_apply(lambda row: pd.Series(level3_asteroid_search(row)), axis=1)\n",
    "\n",
    "        # Prepare Data Frame for CSV presentation\n",
    "        Level_3_Archive_CSV = (\n",
    "            Level_3_Archive_DF.rename(columns={\n",
    "                'proposal_id': 'Proposal', \n",
    "                'observationid': 'Observation', \n",
    "                'dataproducttype': 'Data Type',\n",
    "                'intent': 'Intent',\n",
    "                'instrument_name': 'Instrument',\n",
    "                'energy_bandpassname': 'Filter',\n",
    "                'target_moving': 'Moving', \n",
    "                'position_bounds_spoly': 'Polygon Boundary', \n",
    "                'time_bounds_lower': 'Exposure Start', \n",
    "                'time_bounds_upper': 'Exposure End',\n",
    "                'members': 'Level 2 Members'\n",
    "            })\n",
    "\n",
    "            .assign(**{\n",
    "                'Exposure Start': lambda df: df['Exposure Start'].apply(MJDconversion),                                     # Convert from MJD to 'yyyy-mm-dd HH:MM:SS'\n",
    "                'Exposure End': lambda df: df['Exposure End'].apply(MJDconversion),                                         # Convert from MJD to 'yyyy-mm-dd HH:MM:SS'\n",
    "                'Moving': lambda df: df['Moving'].replace({0: 'No', 1: 'Yes'}),                                             # Adds Yes/No if the observation is 'moving'\n",
    "                'Datalabs': lambda df: df.apply(lambda row: checkDataExists(row['Proposal'], row['Observation']),axis=1),   # Add 'data_exists' column tracking if the img is on Datalabs\n",
    "                'Level 2 Members': lambda df: df['Level 2 Members'].apply(members_string)})                                 # Configure the 'members' format\n",
    "            .sort_values(by='Observation', ascending=True)                                                                  # Sort by Observation ID\n",
    "            .reset_index(drop=True))                                                                                        # Reset the index\n",
    "\n",
    "        # Save CSV\n",
    "        Level_3_Archive_CSV.to_csv(f\"{folderName}/Level3_Asteroid_Search_Full.csv\",index=False)\n",
    "\n",
    "        # Reduce dataframe to only observations with asteroids, save CSV\n",
    "        Level_3_Archive_CSV_Only_Asteroids = Level_3_Archive_CSV[Level_3_Archive_CSV['Asteroids'].str.contains(r'[a-zA-Z]', na=False)].copy()\n",
    "        Level_3_Archive_CSV_Only_Asteroids.to_csv(f\"{folderName}/Level3_Asteroid_Search.csv\",index=False)\n",
    "        \n",
    "        logging.info(f'Finished the Level 3 Search:')\n",
    "        logging.info(f'From {len(Level_3_Archive_CSV)} Searched Observations')\n",
    "        logging.info(f'{len(Level_3_Archive_CSV_Only_Asteroids)} Observations contained atleast 1 Asteroid')\n",
    "        logging.info(f'This took {round((time.time()- start_time)/60)} minutes\\n')\n",
    "           \n",
    "    else:\n",
    "        print(f'Pulling Level 3 Data from CSV:  \"{level3_CSV_Input_Path}\"')\n",
    "        logging.info(f'Pulling Level 3 Data from CSV:  \"{level3_CSV_Input_Path}\"')\n",
    "        \n",
    "        # Convert the CSV into a dataframe\n",
    "        Level_3_Archive_CSV = pd.read_csv(level3_CSV_Input_Path)\n",
    "        \n",
    "        # Reduce dataframe to only observations with asteroids\n",
    "        Level_3_Archive_CSV_Only_Asteroids = Level_3_Archive_CSV[Level_3_Archive_CSV['Asteroids'].str.contains(r'[a-zA-Z]', na=False)].copy()\n",
    "        \n",
    "        logging.info(f'From {len(level3_CSV_Input_Path)} Searched Observations')\n",
    "        logging.info(f'{len(Level_3_Archive_CSV_Only_Asteroids)} Observations contained atleast 1 Asteroid\\n')\n",
    "        logging.info('----------------------------------------------------------------------------/n')\n",
    "        \n",
    "        \n",
    "        \n",
    "    ##### ----- LEVEL 2 ----- #####\n",
    "    \n",
    "\n",
    "    print('Performing Level 2 Search')\n",
    "    logging.info('Performing Level 2 Search\\n')\n",
    "    # Prepare Members for Level 2 inspection\n",
    "    Level_3_Archive_CSV_Only_Asteroids['Level 2 Members'] = Level_3_Archive_CSV_Only_Asteroids['Level 2 Members'].str.split(', ')\n",
    "\n",
    "    Members_DF = Level_3_Archive_CSV_Only_Asteroids.explode('Level 2 Members').reset_index(drop=True)\n",
    "    Members_DF = Members_DF[['Proposal','Level 2 Members']]\n",
    "                     \n",
    "    #convert proposals to numericals\n",
    "    Members_DF['Proposal'] = pd.to_numeric(Members_DF['Proposal'], errors='coerce')\n",
    "    \n",
    "    Members_Range_DF = Members_DF[(Members_DF['Proposal'] >= propRange[0]) & (Members_DF['Proposal'] <= propRange[1])]\n",
    "\n",
    "    if Members_Range_DF.empty:\n",
    "        print(f\"CSV does not contain observations in the range {propRange}\")\n",
    "        logging.info(f'CSV does not contain observations in the range {propRange}\\n')\n",
    "        return\n",
    " \n",
    "    # Remove duplicate entries for 'members' and 'Proposal'\n",
    "    Unique_Members_DF = Members_Range_DF.drop_duplicates().reset_index(drop=True)\n",
    "    Unique_Members_List = Unique_Members_DF['Level 2 Members'].tolist()\n",
    "\n",
    "    # Query the archive data for the level 2 member observations\n",
    "    Level_2_Archive_DF = level2_Archive_Query(Unique_Members_List, instrumentName, dataType, volume, additionalFilters)\n",
    "        \n",
    "    # Generate a dictionary relating proposal ID to asteroids found in level 3 cone search\n",
    "    Proposal_Asteroid_Dict = {}\n",
    "                     \n",
    "    start_time2 = time.time()\n",
    "\n",
    "    for _, row in Level_3_Archive_CSV_Only_Asteroids.iterrows():\n",
    "        proposal = str(row['Proposal'])\n",
    "        asteroids = [asteroid.strip() for asteroid in str(row['Asteroids']).split(', ') if asteroid.strip()]\n",
    "        if proposal not in Proposal_Asteroid_Dict:\n",
    "            # Initialize as a list if the key doesn't exist\n",
    "            Proposal_Asteroid_Dict[proposal] = []  \n",
    "        Proposal_Asteroid_Dict[proposal].extend(asteroids)\n",
    "\n",
    "    # Have the asteroids contained be unique\n",
    "    Proposal_Asteroid_Dict = {key: list(set(value)) for key, value in Proposal_Asteroid_Dict.items()}\n",
    "\n",
    "    # Itterate through the level 2 archive data and check images for asteroids    \n",
    "    Level_2_Archive_DF[['Exposure Length (m)', 'Asteroids', 'Common Name', 'JPL Classification', 'JPL Radius (km)', 'JPL Geo Albedo', \n",
    "                        'JPL Phase Angle (Deg)', 'JPL Sun-Asteroid Dist (AU)','JPL Asteroid-JWST Dist (AU)', 'JPL Midpoint (RA,DEC)', \n",
    "                        'JPL Pos Rates (arcsec/hr)', 'JPL Pos Uncertainty (arcsec)','Readout Mode','Sub Array Configuation','Array Data Type','Dither Type', \n",
    "                        'BrightSky Pix Numbers','Pixel Units','Flux Density (MJy/sr)','Pixel Area (arcsec2/Pix)','JPL Visual Magnitude (mag)',\n",
    "                        'JPL Surface Brightness (mag/arcsec2)','Asteroid Flux (mJy)', 'Flux Error %' ,'Annulus / Img Median', 'S/N', 'JPL Asteroid Distance (pix)',\n",
    "                        'JPL Asteroid Distance (arcsec/exptime)','JPL Center Offset (arcsec)','Aperture NAN Perc','Annulus NAN Perc', \n",
    "                        'Streak Flag']] = Level_2_Archive_DF.progress_apply(lambda row: pd.Series(level2_asteroid_search(row, Proposal_Asteroid_Dict, produce_image, folderName)), axis=1)\n",
    "    \n",
    "    \n",
    "    # Prepare Data Frame for CSV presentation\n",
    "    Level_2_Archive_CSV = (\n",
    "        Level_2_Archive_DF.rename(columns={\n",
    "            'proposal_id': 'Proposal', \n",
    "            'observationid': 'Observation', \n",
    "            'dataproducttype': 'Data Type',\n",
    "            'intent': 'Intent',\n",
    "            'instrument_name': 'Instrument',\n",
    "            'energy_bandpassname': 'Filter', \n",
    "            'target_name': 'Target Name',\n",
    "            'target_moving': 'Target Moving', \n",
    "            'position_bounds_spoly': 'Polygon Boundary', \n",
    "            'time_bounds_lower': 'Exposure Start', \n",
    "            'time_bounds_upper': 'Exposure End',\n",
    "        })\n",
    "        \n",
    "        .assign(**{\n",
    "            'Exposure Start': lambda df: df['Exposure Start'].apply(MJDconversion),                                     # Convert from MJD to 'yyyy-mm-dd HH:MM:SS'\n",
    "            'Exposure End': lambda df: df['Exposure End'].apply(MJDconversion),                                         # Convert from MJD to 'yyyy-mm-dd HH:MM:SS'\n",
    "            'Target Moving': lambda df: df['Target Moving'].replace({0: 'No', 1: 'Yes'}),                                             # Adds Yes/No if the observation is 'moving'\n",
    "            'Datalabs': lambda df: df.apply(lambda row: checkDataExists(row['Proposal'], row['Observation']),axis=1)})  # Add 'data_exists' column based on checkDataExists\n",
    "        .sort_values(by='Observation', ascending=True)                                                                  # Sort by Observation\n",
    "        .reset_index(drop=True))                                                                                   # Reset the index\n",
    "    \n",
    "    # Save CSV\n",
    "    Level_2_Archive_CSV.to_csv(f\"{folderName}/Level2_Asteroid_Search_Full.csv\",index=False)\n",
    "\n",
    "    # Reduce dataframe to only observations with asteroids, save CSV\n",
    "    Level_2_Archive_CSV_Only_Asteroids = Level_2_Archive_CSV[Level_2_Archive_CSV['Asteroids'].str.strip().astype(bool)].copy()\n",
    "    Level_2_Archive_CSV_Only_Asteroids.to_csv(f\"{folderName}/Level2_Asteroid_Search.csv\",index=False)\n",
    "    \n",
    "    logging.info(f'Finished the Level 2 Search:')\n",
    "    logging.info(f'From {len(Level_2_Archive_CSV)} Searched Observations')\n",
    "    logging.info(f'{len(Level_2_Archive_CSV_Only_Asteroids)} Observations contained atleast 1 Asteroid')\n",
    "    logging.info(f'This took {(time.time() - start_time2)/60} minutes\\n')\n",
    "    logging.info(f'For more statistics run \"Asteroid_Analysis\" using this level 2 output')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275f0cb8-5840-4ac2-b160-0a61e2c2acb3",
   "metadata": {},
   "source": [
    "## Run The Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d5ae5beb-0704-47d1-a5e9-3f6894106883",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulling Level 3 Data from CSV:  \"Results/Level3_New_Obs.csv\"\n",
      "Performing Level 2 Search\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a27ef5b955b43f4be326afbd1fa6b38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/135 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 7s, sys: 12.4 s, total: 1min 20s\n",
      "Wall time: 15min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Constants and parameters\n",
    "starting_proposal = 1000\n",
    "ending_proposal = 7000\n",
    "\n",
    "instrument = 'MIRI/IMAGE'\n",
    "data_type = 'image'\n",
    "volume = 'archive'\n",
    "folder_name = 'Results'\n",
    "\n",
    "proposal_range = [starting_proposal,ending_proposal]\n",
    "\n",
    "main(proposal_range, instrument, data_type, volume, folderName=folder_name, level3_CSV_Input_Path = 'Results/Level3_New_Obs.csv', produce_image = True)\n",
    "\n",
    "#Kernel keeps restarting at 1177, 1161\n",
    "# check the uncertainty in position for far and close (1727 streak accidentally picks up a different source)2015 XK95"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Nik_Asteroid",
   "language": "python",
   "name": "nik_asteroid"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
